I'm assuming that many embedding points will be captured

I love the idea of having a zoom parameter


An embedding of the entire document would be equivalent to the entire surface area of the modeled surface

An embedding which is 1% of the surface area of the document is 1% of the surface area of the modeled surface. 

So bringing in the proportional scale of each embedding as defined in the reality itself delivers a variable from which more intelligent matching can be made


This starts to allow us to "take pictures" of the real life entity, create an embedding from the photo, and then place that embedding point onto a coordinate plane

That single picture, the coordinates it embodies on the entity, and the embedding are all stored in the modeled surface.


Maybe: Modeling knowledge in embedding space through photogrammetry 
